from langchain_core.prompts import ChatPromptTemplate, FewShotPromptWithTemplates, FewShotChatMessagePromptTemplate, \
    FewShotPromptTemplate, PromptTemplate, StringPromptTemplate
from langchain_ollama import ChatOllama

base_url = "http://localhost:11434"
model_name = 'deepseek-r1:7b-qwen-distill-q4_K_M'


model_ollama = ChatOllama(
    model = model_name,
    base_url=base_url,
    reasoning=True,#æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
    temperature = 0.8,
)

#====================================1.æ²¡æœ‰
print(model_ollama.invoke("What is 2 ğŸ¦œ 9?"))

# ä½¿ç”¨few-shot
examples = [
    {"input":"2 ğŸ¦œ 2", "output": "4"},
    {"input":"3 ğŸ¦œ 2", "output": "5"}
]
# =================================2. æ–¹æ³•1
prompt_1 = PromptTemplate.from_template(
    "æ ¹æ®{input}',å¾—åˆ°{output}"
)

few_shot_prompts_1 = FewShotPromptWithTemplates(
    examples =examples, # Optional[list[dict]] = None æä¾›çš„æ ·ä¾‹ï¼Œè¿™ä¸ªå’Œexample_selectorå¿…é€‰ä¸€ä¸ª

    # example_selector #: Any = None ç¤ºä¾‹é€‰æ‹©å™¨ï¼Œç”¨äºé€‰æ‹©è¦æ ¼å¼åŒ–åˆ°æç¤ºä¸­çš„ç¤ºä¾‹ã€‚è¦ä¹ˆæä¾›è¿™ä¸ªï¼Œè¦ä¹ˆæä¾›ä¾‹å­ã€‚

    example_prompt = prompt_1,# PromptTemplate  éœ€è¦çš„æç¤ºæ¨¡æ¿


    suffix= PromptTemplate.from_template("æ ¹æ®{input}ï¼Œç»“æœæ˜¯å¤šå°‘ï¼Ÿ"),# StringPromptTemplate æ ·ä¾‹åçš„ä¸»é—®é¢˜ï¼Œé€šå¸¸æ˜¯â€œç”¨æˆ·çœŸæ­£è¦é—®çš„é—®é¢˜â€
    input_variables = ["input"], #å¿…é¡»æ˜¾ç¤ºæŒ‡æ˜å‚æ•°
    # example_separator=# str = "\n\n" ç”¨äºé“¾æ¥prefix,examples,suffixçš„åˆ†å‰²ç¬¦

    prefix =PromptTemplate.from_template("è¯·æ ¹æ®ä»¥ä¸‹ç¤ºä¾‹å›ç­”é—®é¢˜ï¼š"),#: Optional[StringPromptTemplate] = None æ”¾åœ¨exampleçš„å‰é¢ ä¾‹å¦‚ï¼šâ€œè¯·æ ¹æ®ä»¥ä¸‹ç¤ºä¾‹å›ç­”é—®é¢˜ï¼šâ€

    # template_format= , PromptTemplateFormat  æç¤ºæ¨¡æ¿ä½¿ç”¨çš„æ ¼å¼ 'f-string', 'jinja2', 'mustache'."""

    # validate_template= , bool = False  æ˜¯å¦æå‰éªŒè¯æ¨¡æ¿å˜é‡åŒ¹é…ï¼ˆå¯å…³é—­é¿å…å¼€å‘æœŸæŠ¥é”™ï¼‰

)
finale_template_1 = few_shot_prompts_1.format_prompt(input = "4 ğŸ¦œ 4")
model_ollama.invoke(finale_template_1)
# (few_shot_prompts_1 | model_ollama).invoke({"input":"What is 2 ğŸ¦œ 9?"})


# =============================3.æ–¹æ³•3
prompt_2 = ChatPromptTemplate.from_template(
    "æ ¹æ®{input}',å¾—åˆ°{output}"
)

few_shot_prompt_2 = FewShotChatMessagePromptTemplate(
    examples = examples,
    example_prompt = prompt_2,
    # input_variables=[],
    # input_types={},
    # partial_variables={},
)

print(few_shot_prompt_2.invoke({}).to_messages())

final_prompt = ChatPromptTemplate.from_messages(
    [
        {"role": "system","content":"ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹" },
        few_shot_prompt_2,
        {"role": "human","content":"{input}" },

     ]
)
chain = final_prompt | model_ollama
for i in chain.stream({"input":"What is 2 ğŸ¦œ 9?"}):
    print(i.content,end=" ",flush=True)