## 提示模板（prompt templates）
### 1. 定义
用于知道模型响应，帮助模型理解上下文并生成相关且连贯的基于语言的输出
### 2. 输入输出
字典为输入，每个key代表提示模板中要填写的变量  
PromptValue为输出传递给LLM或者ChatModel模型，原因是为了在字符串和消息之间进行切换

### 3. 提示词模板的类型：  

- String PromptTemplates 
用于简单的输出，如只有单个的字符
```python
from langchain_core.prompts import PromptTemplate

prompt_template = PromptTemplate.from_template("Tell me a joke about {topic}")

prompt_template.invoke({"topic": "cats"})
```

- ChatPromptTemplates
```python
from langchain_core.prompts import ChatPromptTemplate

prompt_template = ChatPromptTemplate([
    ("system", "You are a helpful assistant"),
    ("user", "Tell me a joke about {topic}")
])

prompt_template.invoke({"topic": "cats"})
```
- MessagesPlaceholder  消息占位符
```python
prompt_template = ChatPromptTemplate([
    ("system", "You are a helpful assistant"),
    MessagesPlaceholder("msgs")
])
# 需要转换成标准的模板格式
prompt_template.invoke({"msgs": [HumanMessage(content="hi!")]})
# 构建历史消息
messages_to_pass = [
    HumanMessage(content="What's the capital of France?"),
    AIMessage(content="The capital of France is Paris."),
    HumanMessage(content="And what about Germany?")
]
# 填充消息
formatted_prompt = prompt_template.invoke({"msgs": messages_to_pass})
print(formatted_prompt)
```

```python
prompt_template = ChatPromptTemplate([
    ("system", "You are a helpful assistant"),
    ("placeholder", "{msgs}") # <-- This is the changed part
])
```


### 4.声明
##### 方法1. 先声明字符串
使用jinjia , mustache或者f-string写字符串模板，再用`ChatPromptTemplate.from_template`格式化成标准的ChatPromptTemplate类型最后再使用f`format_prompt`或者`format_messages`填入参数并格式化成能输入模型的ChatPromptValue类型，  
**总结**：先from再formate
- 其中jinjia可以写简单的判断等语句，不安全，尽量不用，格式大致如下：
```prompt 如果是jinja2的话可以写控制语句例如：
 {% if language == "英文" %}
 Please translate the following text into English:
 {% else %}
 请将下列文本翻译成中文：
 {% endif %}

 {{ text }}
 ```
- mustache和f-string没区别，一个是{ text } 一个是{text} 因此采用默认的f-string就可以
- 底层还是用的` PromptTemplate.from_template(template, **kwargs)` 最后再包装成ChatPromptTemplate
1.  **from_template**  
- 参数： 
```python
prompt = \
    """
    请将下列文本翻译成{la}
    {text}
    """

prompt_template = ChatPromptTemplate.from_template(
    template=prompt,#模板，f_string {'a'} ，jinja2{{ 'a' }}  mustache {{'a'}}
    partial_variables={"la": "英文"},#预先填充的变量，后面也可以更改
    # template_format="f-string", #不是必填  jinja2可以写控制语句
)
```
- 输出
```
ChatPromptTemplate(
    input_variables=['text'], 
    input_types={}, 
    partial_variables={},
    messages=[
        HumanMessagePromptTemplate(
            prompt=PromptTemplate(
                input_variables=['text'], 
                input_types={},
                partial_variables={'la': '英文'},
                template='\n    请将下列文本翻译成{la}\n    {text}\n    '), 
                additional_kwargs={})])
```
2. format_prompt
左边是str模板中的变量名如{la},{text}等，右边是填入的值，
```prompt_template.format_prompt(la="法语", text="hi") ，```  
输出：
`hatPromptValue(messages=[HumanMessage(content='\n    请将下列文本翻译成法语\n    hi\n    ', additional_kwargs={}, response_metadata={})])`
3. format_messages
左边是str模板中的变量名如{la},{text}等，右边是填入的值，
```prompt_template.format_messages(la="法语", text="hi") ，```  
输出：
`[HumanMessage(content='\n    请将下列文本翻译成法语\n    hi\n    ', additional_kwargs={}, response_metadata={})]`
- 总结 format_prompt 和format_messages都是为了在模板中填入参数，只是最后的用处不一样

    | 方法                          | 返回类型                | 用途                                               | 备注                                |
    | --------------------------- | ------------------- | ------------------------------------------------ |-----------------------------------|
    | `format_prompt(**kwargs)`   | `PromptValue` 对象    | **中间结果**，可以进一步 `.to_string()` 或 `.to_messages()` | 通常用于链式组合 （如 RAG、Router） |
    | `format_messages(**kwargs)` | `List[BaseMessage]` | 直接得到 **LangChain 消息对象** 列表                       | 多用于 `ChatModel.invoke()`          |
    

##### 方法2. 传入消息列表
直接传入openai风格或者liangChain风格的消息列表
```python
prompt_message = ChatPromptTemplate.from_messages(
    messages=[
        ("system", "你是一名全科医学专家,请根据提供的患者信息做出判断。回答必须专业、清晰。"),
        ("human", "患者：45岁男性，持续咳嗽2周，有痰。"),
        ("ai", "【诊断判断】\n可能为急性支气管炎。\n\n【建议检查】\n胸部X光片，痰液检查。\n\n【治疗建议】\n多饮水，止咳药物。"),
        MessagesPlaceholder(variable_name="history"),#消息占位符
         ("human", "{query}")
    ]
    # template_format="f-string",
)
```
输出与from_from_template结构一样，最后能够使用的方法也一样。
**注意**：
1. openai风格{"role":"","content":"""}才是新版本的，上面代码的后面会弃用，
2. openai风格和langchain风格不要混合使用，否则参数传入会失败
3. 他只有两个属性，填入的信息和模板格式f-string等



#### 方法3. 生成字符串
```python
template = "你是一名医生。请根据以下症状提供诊断建议：{symptoms}"

prompt = PromptTemplate(
    input_variables=["symptoms"],#老版本中显示指定变量
    template=template,
)
```
输出：
```
PromptTemplate(input_variables=['symptoms'], input_types={}, partial_variables={}, template='你是一名医生。请根据以下症状提供诊断建议：{symptoms}')
```
填充：
```
filled_prompt = prompt.format(symptoms="咳嗽，发热，头痛")
```
输出:
```
'你是一名医生。请根据以下症状提供诊断建议：咳嗽，发热，头痛'
```
**说明：** 本质上是一个字符串，他没有方法1，2的方法，而且需要显示指定输入参数


### 调用  

一个是使用chain调用,一个是模型直接调用
chain调用不用填充模板,而是调用时填充参数
```
 for i in chain.stream({"text":"今天天气真糟糕，衣服都湿透了"}):
     print(i.content, end="", flush=True)
```

模型调用需要格式化后传入
```python
for i in model.stream(filled_prompt_template):
    print(i.content, end="", flush=True)
```
具体使用案例请参考`2.使用`

### 提示词模板的组合
不同的提示词模板可以组合，
**注意**:  
  1. PipelinePromptTemplate官方不建议使用，而赞成将各个提示链接在一起。 如果使用则不支持form_message的格式，会识别不到变量，都用PromptTemplate.from_template或者 ChatPromptTemplate.from_template  
  2. 如果使用form_message格式，则拼接是使用全是模板相加起来
    ```python
    # def build_prompt(person, example_q, example_a, input):
    #     return introduction_prompt.format_prompt(person=person) + \
    #            example_prompt.format_prompt(example_q=example_q, example_a=example_a) + \
    #            start_prompt.format_prompt(input=input)
    #
    # # 示例输入
    # messages = build_prompt(
    #     person="一名智能机器人",
    #     example_q="你喜欢什么车？",
    #     example_a="我喜欢电动车，比如特斯拉。",
    #     input="你最喜欢哪个社交媒体平台？"
    # )
    ```







